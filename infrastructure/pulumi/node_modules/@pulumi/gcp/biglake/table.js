"use strict";
// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***
Object.defineProperty(exports, "__esModule", { value: true });
exports.Table = void 0;
const pulumi = require("@pulumi/pulumi");
const utilities = require("../utilities");
/**
 * Represents a table.
 *
 * To get more information about Table, see:
 *
 * * [API documentation](https://cloud.google.com/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables)
 * * How-to Guides
 *     * [Manage open source metadata with BigLake Metastore](https://cloud.google.com/bigquery/docs/manage-open-source-metadata#create_tables)
 *
 * ## Example Usage
 *
 * ### Biglake Table
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const catalog = new gcp.biglake.Catalog("catalog", {
 *     name: "my_catalog",
 *     location: "US",
 * });
 * const bucket = new gcp.storage.Bucket("bucket", {
 *     name: "my_bucket",
 *     location: "US",
 *     forceDestroy: true,
 *     uniformBucketLevelAccess: true,
 * });
 * const metadataFolder = new gcp.storage.BucketObject("metadata_folder", {
 *     name: "metadata/",
 *     content: " ",
 *     bucket: bucket.name,
 * });
 * const dataFolder = new gcp.storage.BucketObject("data_folder", {
 *     name: "data/",
 *     content: " ",
 *     bucket: bucket.name,
 * });
 * const database = new gcp.biglake.Database("database", {
 *     name: "my_database",
 *     catalog: catalog.id,
 *     type: "HIVE",
 *     hiveOptions: {
 *         locationUri: pulumi.interpolate`gs://${bucket.name}/${metadataFolder.name}`,
 *         parameters: {
 *             owner: "Alex",
 *         },
 *     },
 * });
 * const table = new gcp.biglake.Table("table", {
 *     name: "my_table",
 *     database: database.id,
 *     type: "HIVE",
 *     hiveOptions: {
 *         tableType: "MANAGED_TABLE",
 *         storageDescriptor: {
 *             locationUri: pulumi.interpolate`gs://${bucket.name}/${dataFolder.name}`,
 *             inputFormat: "org.apache.hadoop.mapred.SequenceFileInputFormat",
 *             outputFormat: "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
 *         },
 *         parameters: {
 *             "spark.sql.create.version": "3.1.3",
 *             "spark.sql.sources.schema.numParts": "1",
 *             transient_lastDdlTime: "1680894197",
 *             "spark.sql.partitionProvider": "catalog",
 *             owner: "John Doe",
 *             "spark.sql.sources.schema.part.0": "{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"age\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]}",
 *             "spark.sql.sources.provider": "iceberg",
 *             provider: "iceberg",
 *         },
 *     },
 * });
 * ```
 *
 * ## Import
 *
 * Table can be imported using any of these accepted formats:
 *
 * * `{{database}}/tables/{{name}}`
 *
 * When using the `pulumi import` command, Table can be imported using one of the formats above. For example:
 *
 * ```sh
 * $ pulumi import gcp:biglake/table:Table default {{database}}/tables/{{name}}
 * ```
 */
class Table extends pulumi.CustomResource {
    /**
     * Get an existing Table resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    static get(name, id, state, opts) {
        return new Table(name, state, Object.assign(Object.assign({}, opts), { id: id }));
    }
    /**
     * Returns true if the given object is an instance of Table.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    static isInstance(obj) {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === Table.__pulumiType;
    }
    constructor(name, argsOrState, opts) {
        let resourceInputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState;
            resourceInputs["createTime"] = state ? state.createTime : undefined;
            resourceInputs["database"] = state ? state.database : undefined;
            resourceInputs["deleteTime"] = state ? state.deleteTime : undefined;
            resourceInputs["etag"] = state ? state.etag : undefined;
            resourceInputs["expireTime"] = state ? state.expireTime : undefined;
            resourceInputs["hiveOptions"] = state ? state.hiveOptions : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["type"] = state ? state.type : undefined;
            resourceInputs["updateTime"] = state ? state.updateTime : undefined;
        }
        else {
            const args = argsOrState;
            resourceInputs["database"] = args ? args.database : undefined;
            resourceInputs["hiveOptions"] = args ? args.hiveOptions : undefined;
            resourceInputs["name"] = args ? args.name : undefined;
            resourceInputs["type"] = args ? args.type : undefined;
            resourceInputs["createTime"] = undefined /*out*/;
            resourceInputs["deleteTime"] = undefined /*out*/;
            resourceInputs["etag"] = undefined /*out*/;
            resourceInputs["expireTime"] = undefined /*out*/;
            resourceInputs["updateTime"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(Table.__pulumiType, name, resourceInputs, opts);
    }
}
exports.Table = Table;
/** @internal */
Table.__pulumiType = 'gcp:biglake/table:Table';
//# sourceMappingURL=table.js.map