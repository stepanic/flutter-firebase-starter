"use strict";
// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***
Object.defineProperty(exports, "__esModule", { value: true });
exports.Task = void 0;
const pulumi = require("@pulumi/pulumi");
const utilities = require("../utilities");
/**
 * A Dataplex task represents the work that you want Dataplex to do on a schedule. It encapsulates code, parameters, and the schedule.
 *
 * To get more information about Task, see:
 *
 * * [API documentation](https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.lakes.tasks)
 * * How-to Guides
 *     * [Official Documentation](https://cloud.google.com/dataplex/docs)
 *
 * ## Example Usage
 *
 * ### Dataplex Task Basic
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const project = gcp.organizations.getProject({});
 * const example = new gcp.dataplex.Lake("example", {
 *     name: "tf-test-lake_91042",
 *     location: "us-central1",
 *     project: "my-project-name",
 * });
 * const exampleTask = new gcp.dataplex.Task("example", {
 *     taskId: "tf-test-task_72490",
 *     location: "us-central1",
 *     lake: example.name,
 *     description: "Test Task Basic",
 *     displayName: "task-basic",
 *     labels: {
 *         count: "3",
 *     },
 *     triggerSpec: {
 *         type: "RECURRING",
 *         disabled: false,
 *         maxRetries: 3,
 *         startTime: "2023-10-02T15:01:23Z",
 *         schedule: "1 * * * *",
 *     },
 *     executionSpec: {
 *         serviceAccount: project.then(project => `${project.number}-compute@developer.gserviceaccount.com`),
 *         project: "my-project-name",
 *         maxJobExecutionLifetime: "100s",
 *         kmsKey: "234jn2kjn42k3n423",
 *     },
 *     spark: {
 *         pythonScriptFile: "gs://dataproc-examples/pyspark/hello-world/hello-world.py",
 *     },
 *     project: "my-project-name",
 * });
 * ```
 * ### Dataplex Task Spark
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * // VPC network
 * const _default = new gcp.compute.Network("default", {
 *     name: "tf-test-workstation-cluster_89605",
 *     autoCreateSubnetworks: true,
 * });
 * const project = gcp.organizations.getProject({});
 * const exampleSpark = new gcp.dataplex.Lake("example_spark", {
 *     name: "tf-test-lake_56730",
 *     location: "us-central1",
 *     project: "my-project-name",
 * });
 * const exampleSparkTask = new gcp.dataplex.Task("example_spark", {
 *     taskId: "tf-test-task_95154",
 *     location: "us-central1",
 *     lake: exampleSpark.name,
 *     triggerSpec: {
 *         type: "ON_DEMAND",
 *     },
 *     description: "task-spark-terraform",
 *     executionSpec: {
 *         serviceAccount: project.then(project => `${project.number}-compute@developer.gserviceaccount.com`),
 *         args: {
 *             TASK_ARGS: "--output_location,gs://spark-job/task-result, --output_format, json",
 *         },
 *     },
 *     spark: {
 *         infrastructureSpec: {
 *             batch: {
 *                 executorsCount: 2,
 *                 maxExecutorsCount: 100,
 *             },
 *             containerImage: {
 *                 image: "test-image",
 *                 javaJars: ["test-java-jars.jar"],
 *                 pythonPackages: ["gs://bucket-name/my/path/to/lib.tar.gz"],
 *                 properties: {
 *                     name: "wrench",
 *                     mass: "1.3kg",
 *                     count: "3",
 *                 },
 *             },
 *             vpcNetwork: {
 *                 networkTags: ["test-network-tag"],
 *                 subNetwork: _default.id,
 *             },
 *         },
 *         fileUris: ["gs://terrafrom-test/test.csv"],
 *         archiveUris: ["gs://terraform-test/test.csv"],
 *         sqlScript: "show databases",
 *     },
 *     project: "my-project-name",
 * });
 * ```
 * ### Dataplex Task Notebook
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * // VPC network
 * const _default = new gcp.compute.Network("default", {
 *     name: "tf-test-workstation-cluster_64336",
 *     autoCreateSubnetworks: true,
 * });
 * const project = gcp.organizations.getProject({});
 * const exampleNotebook = new gcp.dataplex.Lake("example_notebook", {
 *     name: "tf-test-lake_34962",
 *     location: "us-central1",
 *     project: "my-project-name",
 * });
 * const exampleNotebookTask = new gcp.dataplex.Task("example_notebook", {
 *     taskId: "tf-test-task_74000",
 *     location: "us-central1",
 *     lake: exampleNotebook.name,
 *     triggerSpec: {
 *         type: "RECURRING",
 *         schedule: "1 * * * *",
 *     },
 *     executionSpec: {
 *         serviceAccount: project.then(project => `${project.number}-compute@developer.gserviceaccount.com`),
 *         args: {
 *             TASK_ARGS: "--output_location,gs://spark-job-jars-anrajitha/task-result, --output_format, json",
 *         },
 *     },
 *     notebook: {
 *         notebook: "gs://terraform-test/test-notebook.ipynb",
 *         infrastructureSpec: {
 *             batch: {
 *                 executorsCount: 2,
 *                 maxExecutorsCount: 100,
 *             },
 *             containerImage: {
 *                 image: "test-image",
 *                 javaJars: ["test-java-jars.jar"],
 *                 pythonPackages: ["gs://bucket-name/my/path/to/lib.tar.gz"],
 *                 properties: {
 *                     name: "wrench",
 *                     mass: "1.3kg",
 *                     count: "3",
 *                 },
 *             },
 *             vpcNetwork: {
 *                 networkTags: ["test-network-tag"],
 *                 network: _default.id,
 *             },
 *         },
 *         fileUris: ["gs://terraform-test/test.csv"],
 *         archiveUris: ["gs://terraform-test/test.csv"],
 *     },
 *     project: "my-project-name",
 * });
 * ```
 *
 * ## Import
 *
 * Task can be imported using any of these accepted formats:
 *
 * * `projects/{{project}}/locations/{{location}}/lakes/{{lake}}/tasks/{{task_id}}`
 *
 * * `{{project}}/{{location}}/{{lake}}/{{task_id}}`
 *
 * * `{{location}}/{{lake}}/{{task_id}}`
 *
 * When using the `pulumi import` command, Task can be imported using one of the formats above. For example:
 *
 * ```sh
 * $ pulumi import gcp:dataplex/task:Task default projects/{{project}}/locations/{{location}}/lakes/{{lake}}/tasks/{{task_id}}
 * ```
 *
 * ```sh
 * $ pulumi import gcp:dataplex/task:Task default {{project}}/{{location}}/{{lake}}/{{task_id}}
 * ```
 *
 * ```sh
 * $ pulumi import gcp:dataplex/task:Task default {{location}}/{{lake}}/{{task_id}}
 * ```
 */
class Task extends pulumi.CustomResource {
    /**
     * Get an existing Task resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    static get(name, id, state, opts) {
        return new Task(name, state, Object.assign(Object.assign({}, opts), { id: id }));
    }
    /**
     * Returns true if the given object is an instance of Task.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    static isInstance(obj) {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === Task.__pulumiType;
    }
    constructor(name, argsOrState, opts) {
        let resourceInputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState;
            resourceInputs["createTime"] = state ? state.createTime : undefined;
            resourceInputs["description"] = state ? state.description : undefined;
            resourceInputs["displayName"] = state ? state.displayName : undefined;
            resourceInputs["effectiveLabels"] = state ? state.effectiveLabels : undefined;
            resourceInputs["executionSpec"] = state ? state.executionSpec : undefined;
            resourceInputs["executionStatuses"] = state ? state.executionStatuses : undefined;
            resourceInputs["labels"] = state ? state.labels : undefined;
            resourceInputs["lake"] = state ? state.lake : undefined;
            resourceInputs["location"] = state ? state.location : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["notebook"] = state ? state.notebook : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["pulumiLabels"] = state ? state.pulumiLabels : undefined;
            resourceInputs["spark"] = state ? state.spark : undefined;
            resourceInputs["state"] = state ? state.state : undefined;
            resourceInputs["taskId"] = state ? state.taskId : undefined;
            resourceInputs["triggerSpec"] = state ? state.triggerSpec : undefined;
            resourceInputs["uid"] = state ? state.uid : undefined;
            resourceInputs["updateTime"] = state ? state.updateTime : undefined;
        }
        else {
            const args = argsOrState;
            if ((!args || args.executionSpec === undefined) && !opts.urn) {
                throw new Error("Missing required property 'executionSpec'");
            }
            if ((!args || args.triggerSpec === undefined) && !opts.urn) {
                throw new Error("Missing required property 'triggerSpec'");
            }
            resourceInputs["description"] = args ? args.description : undefined;
            resourceInputs["displayName"] = args ? args.displayName : undefined;
            resourceInputs["executionSpec"] = args ? args.executionSpec : undefined;
            resourceInputs["labels"] = args ? args.labels : undefined;
            resourceInputs["lake"] = args ? args.lake : undefined;
            resourceInputs["location"] = args ? args.location : undefined;
            resourceInputs["notebook"] = args ? args.notebook : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["spark"] = args ? args.spark : undefined;
            resourceInputs["taskId"] = args ? args.taskId : undefined;
            resourceInputs["triggerSpec"] = args ? args.triggerSpec : undefined;
            resourceInputs["createTime"] = undefined /*out*/;
            resourceInputs["effectiveLabels"] = undefined /*out*/;
            resourceInputs["executionStatuses"] = undefined /*out*/;
            resourceInputs["name"] = undefined /*out*/;
            resourceInputs["pulumiLabels"] = undefined /*out*/;
            resourceInputs["state"] = undefined /*out*/;
            resourceInputs["uid"] = undefined /*out*/;
            resourceInputs["updateTime"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        const secretOpts = { additionalSecretOutputs: ["effectiveLabels", "pulumiLabels"] };
        opts = pulumi.mergeOptions(opts, secretOpts);
        super(Task.__pulumiType, name, resourceInputs, opts);
    }
}
exports.Task = Task;
/** @internal */
Task.__pulumiType = 'gcp:dataplex/task:Task';
//# sourceMappingURL=task.js.map